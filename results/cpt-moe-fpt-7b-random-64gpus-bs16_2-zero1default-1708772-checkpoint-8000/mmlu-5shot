{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.22,
      "acc_stderr": 0.04163331998932268,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932268
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.03972552884785138,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.03972552884785138
    },
    "hendrycksTest-astronomy": {
      "acc": 0.17763157894736842,
      "acc_stderr": 0.031103182383123398,
      "acc_norm": 0.17763157894736842,
      "acc_norm_stderr": 0.031103182383123398
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.22264150943396227,
      "acc_stderr": 0.0256042334708991,
      "acc_norm": 0.22264150943396227,
      "acc_norm_stderr": 0.0256042334708991
    },
    "hendrycksTest-college_biology": {
      "acc": 0.1875,
      "acc_stderr": 0.032639560491693344,
      "acc_norm": 0.1875,
      "acc_norm_stderr": 0.032639560491693344
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036845,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.20809248554913296,
      "acc_stderr": 0.030952890217749884,
      "acc_norm": 0.20809248554913296,
      "acc_norm_stderr": 0.030952890217749884
    },
    "hendrycksTest-college_physics": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.04280105837364396,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364396
    },
    "hendrycksTest-computer_security": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036845,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2680851063829787,
      "acc_stderr": 0.028957342788342347,
      "acc_norm": 0.2680851063829787,
      "acc_norm_stderr": 0.028957342788342347
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.042270544512322
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.23448275862068965,
      "acc_stderr": 0.035306258743465914,
      "acc_norm": 0.23448275862068965,
      "acc_norm_stderr": 0.035306258743465914
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.20105820105820105,
      "acc_stderr": 0.020641810782370172,
      "acc_norm": 0.20105820105820105,
      "acc_norm_stderr": 0.020641810782370172
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.18253968253968253,
      "acc_stderr": 0.03455071019102147,
      "acc_norm": 0.18253968253968253,
      "acc_norm_stderr": 0.03455071019102147
    },
    "hendrycksTest-global_facts": {
      "acc": 0.18,
      "acc_stderr": 0.038612291966536934,
      "acc_norm": 0.18,
      "acc_norm_stderr": 0.038612291966536934
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3032258064516129,
      "acc_stderr": 0.02614868593067175,
      "acc_norm": 0.3032258064516129,
      "acc_norm_stderr": 0.02614868593067175
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2955665024630542,
      "acc_stderr": 0.032104944337514575,
      "acc_norm": 0.2955665024630542,
      "acc_norm_stderr": 0.032104944337514575
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816508,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816508
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.296969696969697,
      "acc_stderr": 0.035679697722680474,
      "acc_norm": 0.296969696969697,
      "acc_norm_stderr": 0.035679697722680474
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.20707070707070707,
      "acc_stderr": 0.028869778460267063,
      "acc_norm": 0.20707070707070707,
      "acc_norm_stderr": 0.028869778460267063
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.35233160621761656,
      "acc_stderr": 0.03447478286414359,
      "acc_norm": 0.35233160621761656,
      "acc_norm_stderr": 0.03447478286414359
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.23076923076923078,
      "acc_stderr": 0.021362027725222717,
      "acc_norm": 0.23076923076923078,
      "acc_norm_stderr": 0.021362027725222717
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.27037037037037037,
      "acc_stderr": 0.027080372815145658,
      "acc_norm": 0.27037037037037037,
      "acc_norm_stderr": 0.027080372815145658
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.2605042016806723,
      "acc_stderr": 0.028510251512341926,
      "acc_norm": 0.2605042016806723,
      "acc_norm_stderr": 0.028510251512341926
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.17880794701986755,
      "acc_stderr": 0.031287448506007225,
      "acc_norm": 0.17880794701986755,
      "acc_norm_stderr": 0.031287448506007225
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.22752293577981653,
      "acc_stderr": 0.017974463578776502,
      "acc_norm": 0.22752293577981653,
      "acc_norm_stderr": 0.017974463578776502
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4722222222222222,
      "acc_stderr": 0.0340470532865388,
      "acc_norm": 0.4722222222222222,
      "acc_norm_stderr": 0.0340470532865388
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.030587591351604246,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.030587591351604246
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.25316455696202533,
      "acc_stderr": 0.0283046579430353,
      "acc_norm": 0.25316455696202533,
      "acc_norm_stderr": 0.0283046579430353
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2825112107623318,
      "acc_stderr": 0.03021683101150878,
      "acc_norm": 0.2825112107623318,
      "acc_norm_stderr": 0.03021683101150878
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.25190839694656486,
      "acc_stderr": 0.03807387116306086,
      "acc_norm": 0.25190839694656486,
      "acc_norm_stderr": 0.03807387116306086
    },
    "hendrycksTest-international_law": {
      "acc": 0.33884297520661155,
      "acc_stderr": 0.0432076780753667,
      "acc_norm": 0.33884297520661155,
      "acc_norm_stderr": 0.0432076780753667
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.040191074725573483,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.040191074725573483
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.2822085889570552,
      "acc_stderr": 0.03536117886664743,
      "acc_norm": 0.2822085889570552,
      "acc_norm_stderr": 0.03536117886664743
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.17857142857142858,
      "acc_stderr": 0.036352091215778065,
      "acc_norm": 0.17857142857142858,
      "acc_norm_stderr": 0.036352091215778065
    },
    "hendrycksTest-management": {
      "acc": 0.1650485436893204,
      "acc_stderr": 0.036756688322331886,
      "acc_norm": 0.1650485436893204,
      "acc_norm_stderr": 0.036756688322331886
    },
    "hendrycksTest-marketing": {
      "acc": 0.19658119658119658,
      "acc_stderr": 0.02603538609895129,
      "acc_norm": 0.19658119658119658,
      "acc_norm_stderr": 0.02603538609895129
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.23499361430395913,
      "acc_stderr": 0.015162024152278434,
      "acc_norm": 0.23499361430395913,
      "acc_norm_stderr": 0.015162024152278434
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.23121387283236994,
      "acc_stderr": 0.022698657167855716,
      "acc_norm": 0.23121387283236994,
      "acc_norm_stderr": 0.022698657167855716
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.23202614379084968,
      "acc_stderr": 0.024170840879341016,
      "acc_norm": 0.23202614379084968,
      "acc_norm_stderr": 0.024170840879341016
    },
    "hendrycksTest-philosophy": {
      "acc": 0.1832797427652733,
      "acc_stderr": 0.02197419884826581,
      "acc_norm": 0.1832797427652733,
      "acc_norm_stderr": 0.02197419884826581
    },
    "hendrycksTest-prehistory": {
      "acc": 0.21604938271604937,
      "acc_stderr": 0.022899162918445806,
      "acc_norm": 0.21604938271604937,
      "acc_norm_stderr": 0.022899162918445806
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.24113475177304963,
      "acc_stderr": 0.02551873104953777,
      "acc_norm": 0.24113475177304963,
      "acc_norm_stderr": 0.02551873104953777
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2457627118644068,
      "acc_stderr": 0.010996156635142692,
      "acc_norm": 0.2457627118644068,
      "acc_norm_stderr": 0.010996156635142692
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.45588235294117646,
      "acc_stderr": 0.030254372573976694,
      "acc_norm": 0.45588235294117646,
      "acc_norm_stderr": 0.030254372573976694
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.25,
      "acc_stderr": 0.01751781884501444,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.01751781884501444
    },
    "hendrycksTest-public_relations": {
      "acc": 0.21818181818181817,
      "acc_stderr": 0.03955932861795833,
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03955932861795833
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2612244897959184,
      "acc_stderr": 0.028123429335142783,
      "acc_norm": 0.2612244897959184,
      "acc_norm_stderr": 0.028123429335142783
    },
    "hendrycksTest-sociology": {
      "acc": 0.24378109452736318,
      "acc_stderr": 0.03036049015401465,
      "acc_norm": 0.24378109452736318,
      "acc_norm_stderr": 0.03036049015401465
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322674,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322674
    },
    "hendrycksTest-virology": {
      "acc": 0.1686746987951807,
      "acc_stderr": 0.029152009627856544,
      "acc_norm": 0.1686746987951807,
      "acc_norm_stderr": 0.029152009627856544
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3216374269005848,
      "acc_stderr": 0.03582529442573122,
      "acc_norm": 0.3216374269005848,
      "acc_norm_stderr": 0.03582529442573122
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "llama-moe-causal",
    "model_args": "pretrained=/mnt/petrelfs/zhutong/smoe/outputs/cpt-moe-fpt-7b-random-64gpus-bs16_2-zero1default-1708772/checkpoint-8000,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "2",
    "batch_sizes": [],
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}