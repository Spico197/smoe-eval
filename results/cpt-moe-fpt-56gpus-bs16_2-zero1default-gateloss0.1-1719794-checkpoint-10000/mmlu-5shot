{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-anatomy": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.0391545063041425,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.0391545063041425
    },
    "hendrycksTest-astronomy": {
      "acc": 0.23026315789473684,
      "acc_stderr": 0.03426059424403165,
      "acc_norm": 0.23026315789473684,
      "acc_norm_stderr": 0.03426059424403165
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322695
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.028254200344438665,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.028254200344438665
    },
    "hendrycksTest-college_biology": {
      "acc": 0.22916666666666666,
      "acc_stderr": 0.035146974678623884,
      "acc_norm": 0.22916666666666666,
      "acc_norm_stderr": 0.035146974678623884
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384741,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384741
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2832369942196532,
      "acc_stderr": 0.03435568056047874,
      "acc_norm": 0.2832369942196532,
      "acc_norm_stderr": 0.03435568056047874
    },
    "hendrycksTest-college_physics": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.042801058373643966,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.042801058373643966
    },
    "hendrycksTest-computer_security": {
      "acc": 0.23,
      "acc_stderr": 0.042295258468165044,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.042295258468165044
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3659574468085106,
      "acc_stderr": 0.031489558297455304,
      "acc_norm": 0.3659574468085106,
      "acc_norm_stderr": 0.031489558297455304
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2620689655172414,
      "acc_stderr": 0.03664666337225256,
      "acc_norm": 0.2620689655172414,
      "acc_norm_stderr": 0.03664666337225256
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.26455026455026454,
      "acc_stderr": 0.022717467897708617,
      "acc_norm": 0.26455026455026454,
      "acc_norm_stderr": 0.022717467897708617
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04285714285714281,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04285714285714281
    },
    "hendrycksTest-global_facts": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816508,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816508
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.25806451612903225,
      "acc_stderr": 0.024892469172462836,
      "acc_norm": 0.25806451612903225,
      "acc_norm_stderr": 0.024892469172462836
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.29064039408866993,
      "acc_stderr": 0.03194740072265541,
      "acc_norm": 0.29064039408866993,
      "acc_norm_stderr": 0.03194740072265541
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2606060606060606,
      "acc_stderr": 0.03427743175816524,
      "acc_norm": 0.2606060606060606,
      "acc_norm_stderr": 0.03427743175816524
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3383838383838384,
      "acc_stderr": 0.03371124142626303,
      "acc_norm": 0.3383838383838384,
      "acc_norm_stderr": 0.03371124142626303
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.29533678756476683,
      "acc_stderr": 0.0329229663915514,
      "acc_norm": 0.29533678756476683,
      "acc_norm_stderr": 0.0329229663915514
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.33076923076923076,
      "acc_stderr": 0.02385479568097113,
      "acc_norm": 0.33076923076923076,
      "acc_norm_stderr": 0.02385479568097113
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.26296296296296295,
      "acc_stderr": 0.026842057873833706,
      "acc_norm": 0.26296296296296295,
      "acc_norm_stderr": 0.026842057873833706
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3403361344537815,
      "acc_stderr": 0.030778057422931673,
      "acc_norm": 0.3403361344537815,
      "acc_norm_stderr": 0.030778057422931673
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.304635761589404,
      "acc_stderr": 0.03757949922943342,
      "acc_norm": 0.304635761589404,
      "acc_norm_stderr": 0.03757949922943342
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.28440366972477066,
      "acc_stderr": 0.019342036587702588,
      "acc_norm": 0.28440366972477066,
      "acc_norm_stderr": 0.019342036587702588
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4722222222222222,
      "acc_stderr": 0.0340470532865388,
      "acc_norm": 0.4722222222222222,
      "acc_norm_stderr": 0.0340470532865388
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.030587591351604236,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.030587591351604236
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.2869198312236287,
      "acc_stderr": 0.029443773022594703,
      "acc_norm": 0.2869198312236287,
      "acc_norm_stderr": 0.029443773022594703
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2062780269058296,
      "acc_stderr": 0.02715715047956382,
      "acc_norm": 0.2062780269058296,
      "acc_norm_stderr": 0.02715715047956382
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.20610687022900764,
      "acc_stderr": 0.035477710041594654,
      "acc_norm": 0.20610687022900764,
      "acc_norm_stderr": 0.035477710041594654
    },
    "hendrycksTest-international_law": {
      "acc": 0.3305785123966942,
      "acc_stderr": 0.04294340845212093,
      "acc_norm": 0.3305785123966942,
      "acc_norm_stderr": 0.04294340845212093
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.2962962962962963,
      "acc_stderr": 0.04414343666854932,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.04414343666854932
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.25766871165644173,
      "acc_stderr": 0.03436150827846917,
      "acc_norm": 0.25766871165644173,
      "acc_norm_stderr": 0.03436150827846917
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.16071428571428573,
      "acc_stderr": 0.03485946096475741,
      "acc_norm": 0.16071428571428573,
      "acc_norm_stderr": 0.03485946096475741
    },
    "hendrycksTest-management": {
      "acc": 0.24271844660194175,
      "acc_stderr": 0.04245022486384495,
      "acc_norm": 0.24271844660194175,
      "acc_norm_stderr": 0.04245022486384495
    },
    "hendrycksTest-marketing": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.029343114798094462,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.029343114798094462
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.2,
      "acc_stderr": 0.040201512610368466,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.040201512610368466
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.26309067688378035,
      "acc_stderr": 0.015745497169049046,
      "acc_norm": 0.26309067688378035,
      "acc_norm_stderr": 0.015745497169049046
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.24277456647398843,
      "acc_stderr": 0.0230836585869842,
      "acc_norm": 0.24277456647398843,
      "acc_norm_stderr": 0.0230836585869842
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808836,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808836
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2679738562091503,
      "acc_stderr": 0.025360603796242557,
      "acc_norm": 0.2679738562091503,
      "acc_norm_stderr": 0.025360603796242557
    },
    "hendrycksTest-philosophy": {
      "acc": 0.26366559485530544,
      "acc_stderr": 0.02502553850053234,
      "acc_norm": 0.26366559485530544,
      "acc_norm_stderr": 0.02502553850053234
    },
    "hendrycksTest-prehistory": {
      "acc": 0.25,
      "acc_stderr": 0.02409347123262133,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.02409347123262133
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.25177304964539005,
      "acc_stderr": 0.025892151156709405,
      "acc_norm": 0.25177304964539005,
      "acc_norm_stderr": 0.025892151156709405
    },
    "hendrycksTest-professional_law": {
      "acc": 0.24967405475880053,
      "acc_stderr": 0.011054538377832322,
      "acc_norm": 0.24967405475880053,
      "acc_norm_stderr": 0.011054538377832322
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4485294117647059,
      "acc_stderr": 0.030211479609121593,
      "acc_norm": 0.4485294117647059,
      "acc_norm_stderr": 0.030211479609121593
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.23366013071895425,
      "acc_stderr": 0.017119158496044503,
      "acc_norm": 0.23366013071895425,
      "acc_norm_stderr": 0.017119158496044503
    },
    "hendrycksTest-public_relations": {
      "acc": 0.21818181818181817,
      "acc_stderr": 0.03955932861795833,
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03955932861795833
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4,
      "acc_stderr": 0.031362502409358936,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.031362502409358936
    },
    "hendrycksTest-sociology": {
      "acc": 0.25870646766169153,
      "acc_stderr": 0.030965903123573,
      "acc_norm": 0.25870646766169153,
      "acc_norm_stderr": 0.030965903123573
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.26,
      "acc_stderr": 0.044084400227680794,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.044084400227680794
    },
    "hendrycksTest-virology": {
      "acc": 0.26506024096385544,
      "acc_stderr": 0.03436024037944967,
      "acc_norm": 0.26506024096385544,
      "acc_norm_stderr": 0.03436024037944967
    },
    "hendrycksTest-world_religions": {
      "acc": 0.25146198830409355,
      "acc_stderr": 0.033275044238468436,
      "acc_norm": 0.25146198830409355,
      "acc_norm_stderr": 0.033275044238468436
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "llama-moe-causal",
    "model_args": "pretrained=/mnt/petrelfs/zhutong/smoe/outputs/cpt-moe-fpt-56gpus-bs16_2-zero1default-gateloss0.1-1719794/checkpoint-10000,use_accelerate=True",
    "num_fewshot": 5,
    "batch_size": "2",
    "batch_sizes": [],
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}